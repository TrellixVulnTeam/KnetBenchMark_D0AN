{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Knet, JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/KUFS/scratch/okirnap/benchmark/data/CNN/CNN_data.jld\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const LR = 0.01\n",
    "const MOMENTUM = 0.9\n",
    "const BATCHSIZE = 64\n",
    "const ddir = \"/KUFS/scratch/okirnap/benchmark/data/CNN/CNN_data.jld\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "oparams (generic function with 4 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimization parameter creator for parameters\n",
    "oparams{T<:Number}(::KnetArray{T},otype; o...)=otype(;o...)\n",
    "oparams{T<:Number}(::Array{T},otype; o...)=otype(;o...)\n",
    "oparams(a::Associative,otype; o...)=Dict([ k=>oparams(v,otype;o...) for (k,v) in a ])\n",
    "oparams(a,otype; o...)=map(x->oparams(x,otype;o...), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv_bias (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply regular convolution for a given input \n",
    "conv_bias(input, weight, bias; padding=1) = conv4(weight, input;padding=padding) .+ bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass of a model\n",
    "function forward(model, input, ygold; odrop=(0.25, 0.25, 0.5))\n",
    "    rel1  = relu.(conv_bias(input, model[1], model[2]))\n",
    "    pool1 = pool(conv_bias(rel1, model[3], model[4]))\n",
    "    rel2  = relu.(pool1) \n",
    "    drop1 = dropout(rel2, odrop[1])\n",
    "    rel3  = relu.(conv_bias(drop1, model[5], model[6]))\n",
    "    pool2 = pool(conv_bias(rel3, model[7], model[8]))\n",
    "    rel4  = relu.(pool2)\n",
    "    drop2 = dropout(rel4, odrop[2])\n",
    "    flaten = mat(drop2)\n",
    "    fc1 = relu.(model[9] * flaten .+ model[10])\n",
    "    drop3 = dropout(fc1, odrop[3])\n",
    "    ypred = model[11] * drop3 .+ model[12]\n",
    "    return ypred\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logprob (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To calculate the correct logprobabilities for given set of outputs\n",
    "function logprob(outputs, ypred)\n",
    "    nrows, ncols = size(ypred)\n",
    "    index = similar(outputs)\n",
    "    @inbounds for i in 1:length(outputs)\n",
    "        index[i] = (outputs[i] + 1) + (i-1)*nrows\n",
    "    end\n",
    "    o1 = logp(ypred, 1)\n",
    "    o2 = o1[index]\n",
    "    o3 = sum(o2)\n",
    "    return o3\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnnloss (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To calculate the loss value for given input x\n",
    "function cnnloss(model, x, ygold; odrop=(0.25, 0.25, 0.5))\n",
    "    ypred = forward(model, x, ygold; odrop=(0.25, 0.25, 0.5))\n",
    "    total = logprob(ygold, ypred)\n",
    "    count = length(ygold)\n",
    "    return -total / count\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnngrad = grad(cnnloss) # Knet takes care of grads :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minibatch (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function minibatch(X, y; batchsize=BATCHSIZE)\n",
    "    indix = randperm(length(y))\n",
    "    data = Any[]\n",
    "    for i in 1:batchsize:size(X)[4]\n",
    "        j = min(i+batchsize-1, size(X)[4])\n",
    "        batch = X[:, :, :, indix[i:j]]\n",
    "        ygold = y[indix[i:j]]\n",
    "        trial = (gpu()>=0 ? KnetArray{Float32}(batch) : batch)\n",
    "        push!(data,(trial, ygold))\n",
    "    end\n",
    "    return data\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measures the model's performance based on given correct y labels, X input\n",
    "function accuracy(model, X, y)\n",
    "    data = minibatch(X, y)\n",
    "    ntot = ncorrect = 0\n",
    "    for (x, ygold) in data\n",
    "        ypred = forward(model, x, ygold; odrop=(0, 0, 0))\n",
    "        nrows, ncols = size(ypred)\n",
    "        index = similar(ygold)\n",
    "        @inbounds for i in 1:length(ygold)\n",
    "            index[i] = (ygold[i] + 1) + (i-1)*nrows\n",
    "        end\n",
    "        ntot += length(index)\n",
    "        ncorrect += (sum(reshape(findmax(Array((logp(ypred, 1))), 1)[2], length(ygold)) .== index))\n",
    "    end\n",
    "    return ncorrect /ntot   \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To calculate the gradients and update the model\n",
    "function train!(model, data, opts)\n",
    "    @time for (x, y) in data\n",
    "        grads = cnngrad(model, x, y)\n",
    "        update!(model, grads, opts)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initmodel (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initmodel(;init=xavier, ftype=Float32, data=nothing)\n",
    "    f = (gpu()>=0 ? KnetArray{ftype} : Array{ftype})\n",
    "    model = Any[]\n",
    "    push!(model, f(init(3, 3, 3, 50))) # conv1\n",
    "    push!(model, f(zeros(1,1, 50, 1))) # bias1\n",
    "    \n",
    "    push!(model, f(init(3, 3, 50, 50))) # conv2\n",
    "    push!(model, f(zeros(1,1, 50,1))) # bias2\n",
    "    \n",
    "    push!(model, f(init(3, 3, 50, 100))) # conv3\n",
    "    push!(model, f(zeros(1,1,100,1))) # bias3\n",
    "\n",
    "    push!(model, f(init(3,3, 100, 100))) # conv4\n",
    "    push!(model, f(zeros(1,1,100,1))) # bias4\n",
    "\n",
    "    push!(model, f(init(512, 6400))) # fc1\n",
    "    push!(model, f(zeros(512, 1))) # bias\n",
    "\n",
    "    push!(model, f(init(10, 512))) #soft_w\n",
    "    push!(model, f(zeros(10, 1)))  # soft_b\n",
    "    if data !=nothing\n",
    "        (x, y) = data\n",
    "        cnngrad(model, x, y) # To help julia for compilation\n",
    "    end\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function main()\n",
    "    x_train2, x_test2, y_train, y_test =\n",
    "            JLD.load(ddir, \"x_train\", \"x_test\", \"y_train\", \"y_test\")\n",
    "    x_train = permutedims(x_train2, [3,4,2,1])\n",
    "    x_test  = permutedims(x_test2, [3,4,2,1])\n",
    "    data = minibatch(x_train, y_train)\n",
    "    model = initmodel(;data=data[1])\n",
    "    opts = oparams(model, Momentum;lr=LR, gclip=0, gamma=MOMENTUM)\n",
    "    for epoch =1:10\n",
    "        shuffle!(data)\n",
    "        train!(model, data, opts)\n",
    "        acc1 = accuracy(model, x_train, y_train)\n",
    "        acc2 = accuracy(model, x_test, y_test)\n",
    "        println(\":epoch $epoch :Train $acc1 :Test $acc2\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17.842771 seconds (2.40 M allocations: 102.175 MiB, 10.45% gc time)\n",
      ":epoch 1 :Train 0.44912 :Test 0.451\n",
      " 16.849041 seconds (2.22 M allocations: 91.725 MiB, 6.56% gc time)\n",
      ":epoch 2 :Train 0.60604 :Test 0.5898\n",
      " 17.478889 seconds (2.20 M allocations: 91.103 MiB, 11.51% gc time)\n",
      ":epoch 3 :Train 0.69682 :Test 0.6695\n",
      " 20.479341 seconds (2.21 M allocations: 91.235 MiB, 22.87% gc time)\n",
      ":epoch 4 :Train 0.74088 :Test 0.7137\n",
      " 21.005613 seconds (2.21 M allocations: 91.217 MiB, 24.85% gc time)\n",
      ":epoch 5 :Train 0.78672 :Test 0.7345\n",
      " 19.847084 seconds (2.21 M allocations: 91.232 MiB, 21.01% gc time)\n",
      ":epoch 6 :Train 0.81318 :Test 0.7425\n",
      " 23.739767 seconds (2.21 M allocations: 91.185 MiB, 32.54% gc time)\n",
      ":epoch 7 :Train 0.84502 :Test 0.7558"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
